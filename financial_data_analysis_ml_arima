{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"196mk_c0lzqXqiGKI_AfPgFyxZSD9hGGV","timestamp":1689605084852},{"file_id":"1hcjxNolReswZqHpCYYNzMErt2pkW1u8x","timestamp":1689351737967},{"file_id":"1hlYGEYpQNGIfgNeZizq0qsvTp6uFdIV3","timestamp":1689350767469},{"file_id":"1hGyO_MVK2R22IUHWmlBGFaoMhwbJG30A","timestamp":1689350245898}],"authorship_tag":"ABX9TyP1BICy241K7rs2K8onvYKu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"cfttmZ4qD_kj","colab":{"base_uri":"https://localhost:8080/","height":698},"executionInfo":{"status":"ok","timestamp":1690829374351,"user_tz":240,"elapsed":17459,"user":{"displayName":"Aditya Jain","userId":"11504618631135712108"}},"outputId":"6ed9de0d-b375-4dd7-b87a-c6252ef7c354"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-8ef5ba26-6e3c-4677-abbd-e2b8a946d8e4\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-8ef5ba26-6e3c-4677-abbd-e2b8a946d8e4\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving detrended_normalized_financial_data_2014_2023 - all_detrended_normalized (1).csv to detrended_normalized_financial_data_2014_2023 - all_detrended_normalized (1).csv\n","           date  compound  compound_detrended  VCDAX_V_detrended  VCDAX_V_N  \\\n","2177  5/17/2023    0.6705            0.624579          -2.899964  -0.346201   \n","2178  5/24/2023   -0.6513           -0.697221          -1.340144  -0.159988   \n","2179  5/25/2023    0.7717            0.725779          -1.543324  -0.184244   \n","2180  5/26/2023   -0.4588           -0.504721           1.869497   0.223183   \n","2181  5/19/2023   -0.5106           -0.556521          -2.567683  -0.306533   \n","\n","      VCSAX_V_detrended  VCSAX_V_N  VENAX_V_detrended  VENAX_V_N  \\\n","2177          -0.038722  -0.005571           1.721613   0.144542   \n","2178           0.238856   0.034365          -4.875521  -0.409335   \n","2179          -0.949566  -0.136616          -1.191654  -0.100048   \n","2180           0.026012   0.003742          -1.351787  -0.113492   \n","2181          -2.216410  -0.318880          -2.829920  -0.237592   \n","\n","      VFAIX_V_detrended  ...  VTCAX_P_detrended  VTCAX_P_N  VCSAX_P_detrended  \\\n","2177           3.638679  ...           0.748437   0.385716           0.153213   \n","2178           1.990520  ...           0.781301   0.402652          -2.565755   \n","2179           1.799361  ...           0.654164   0.337131          -3.044724   \n","2180           1.508201  ...           1.547028   0.797279          -2.853692   \n","2181          -0.359958  ...           1.169891   0.602917          -0.152660   \n","\n","      VCSAX_P_N  VENAX_P_detrended  VENAX_P_N  VCDAX_P_detrended  VCDAX_P_N  \\\n","2177   0.077659          -2.295099  -0.862382           1.485902   0.314563   \n","2178  -1.300495          -0.749001  -0.281437          -0.294101  -0.062261   \n","2179  -1.543268          -1.842903  -0.692469          -1.244104  -0.263375   \n","2180  -1.446441          -2.036806  -0.765328           1.425893   0.301860   \n","2181  -0.077378          -1.490708  -0.560132           1.625890   0.344199   \n","\n","      season    year  \n","2177  spring  2023.0  \n","2178  spring  2023.0  \n","2179  spring  2023.0  \n","2180  spring  2023.0  \n","2181  spring  2023.0  \n","\n","[5 rows x 49 columns]\n"]}],"source":["from google.colab import files\n","import pandas as pd\n","\n","# Upload CSV file from local machine\n","uploaded = files.upload()\n","\n","# Get the file name\n","file_name = next(iter(uploaded))\n","\n","# Read the CSV file into a Pandas DataFrame\n","df = pd.read_csv(file_name)\n","\n","df = df.drop(df.tail(2).index)\n","\n","print(df.tail())"]},{"cell_type":"markdown","source":["# 10-Year Analysis"],"metadata":{"id":"YKOoOr5J9THx"}},{"cell_type":"markdown","source":["**Granger Causality**"],"metadata":{"id":"61gadMMbsOdT"}},{"cell_type":"code","source":["from statsmodels.tsa.stattools import grangercausalitytests\n","import pandas as pd\n","from google.colab import files\n","\n","# Create an empty DataFrame to store the Granger causality test results\n","granger_results_l = pd.DataFrame(columns=['fund name', 'lag', 'p-value'])\n","\n","# Define the list of columns to calculate correlations and perform Granger causality test with\n","columns_to_analyze = [\n","    'VCDAX_V_N','VCSAX_V_N', 'VENAX_V_N','VFAIX_V_N', 'VGSLX_V_N','VGHCX_V_N',\t'VITAX_V_N',\t'VSPVX_V_N','VTCAX_V_N','VUIAX_V_N', 'VINAX_V_N',\t'VUIAX_P_N',\t'VGHCX_P_N', 'VFAIX_P_N', 'VSPVX_P_N', 'VITAX_P_N', 'VGSLX_P_N','VINAX_P_N',\t'VTCAX_P_N','VCSAX_P_N','VENAX_P_N','VCDAX_P_N'\n","    ]\n","\n","# Perform Granger causality test for each column\n","for column in columns_to_analyze:\n","\n","  # Perform Granger causality test with different lags\n","  max_lag = 5  # Maximum lag to test\n","  granger_test_results_l = grangercausalitytests(df[[column, 'compound']], maxlag=max_lag, verbose=False)\n","\n","  # Iterate over the lag values and add significant Granger causality results to the DataFrame\n","  for lag in range(1, max_lag + 1):\n","      p_value = granger_test_results_l[lag][0]['ssr_ftest'][1]\n","      if p_value  > 0:\n","          result = {\n","              'fund name': column,\n","              'lag': lag,\n","              'p-value': p_value\n","          }\n","          granger_results_l = pd.concat([granger_results_l, pd.DataFrame(result, index=[0])])\n","\n","\n","# Export the Granger causality test results DataFrame to a CSV file\n","granger_results_l.to_csv('granger_results_longer.csv', index=False)\n","files.download('granger_results_longer.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"-FUFy500tByG","executionInfo":{"status":"ok","timestamp":1690829382735,"user_tz":240,"elapsed":6238,"user":{"displayName":"Aditya Jain","userId":"11504618631135712108"}},"outputId":"e8d71c12-2241-412c-b02b-7742d7d3c432"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_dd2c4aa9-e31c-40b8-9fdb-05940b2de756\", \"granger_results_longer.csv\", 3477)"]},"metadata":{}}]},{"cell_type":"markdown","source":["**Pearson R Correlation**"],"metadata":{"id":"VHqKi7kesRM9"}},{"cell_type":"code","source":["from scipy.stats import pearsonr\n","import pandas as pd\n","\n","# Create an empty DataFrame to store the statistically significant results\n","pearson_results_l = pd.DataFrame(columns=['fund name', 'p-value', 'correlation coefficient'])\n","\n","# Perform Pearson correlation analysis for each column\n","for column in columns_to_analyze:\n","    try:\n","        # Extract the relevant series and the target variable\n","        X = df['compound']\n","        y = df[column]\n","\n","        # Calculate Pearson correlation coefficient and p-value\n","        correlation, p_value = pearsonr(X, y)\n","\n","        # Add the significant result to the DataFrame\n","        result = {\n","            'fund name': column,\n","            'p-value': p_value,\n","            'correlation coefficient': correlation\n","            }\n","        pearson_results_l = pd.concat([pearson_results_l, pd.DataFrame(result, index=[0])])\n","    except ValueError:\n","        # Handle the ValueError when x and y have length less than 2\n","        pass\n","\n","# Export the significant results DataFrame to a CSV file\n","pearson_results_l.to_csv('pearson_results_l.csv', index=False)\n","files.download('pearson_results_l.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"dF806Oi5tCOh","executionInfo":{"status":"ok","timestamp":1689887080759,"user_tz":240,"elapsed":256,"user":{"displayName":"Aditya Jain","userId":"11504618631135712108"}},"outputId":"39e1cda2-6b61-4adf-d94b-2660aaf9402c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_45ec5c2f-ed55-4aa2-9f88-371997a2d4bc\", \"pearson_results_l.csv\", 1149)"]},"metadata":{}}]},{"cell_type":"markdown","source":["**Machine Learning**"],"metadata":{"id":"MMLrFwz5sX5N"}},{"cell_type":"code","source":["from sklearn.svm import SVR\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","import lightgbm as lgb\n","import xgboost as xgb\n","from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n","import pandas as pd\n","from google.colab import files\n","\n","# Create an empty DataFrame to store the ML results\n","ml_results_l = pd.DataFrame(columns=['fund name', 'model', 'mape', 'rmse'])\n","\n","# Run machine learning models on each column\n","for column in columns_to_analyze:\n","    X = df['compound']\n","    y = df[column]\n","\n","    if X.empty or y.empty:\n","        continue\n","\n","    # Support Vector Regression\n","    svr = SVR()\n","    svr.fit(X.values.reshape(-1, 1), y.values.reshape(-1, 1))\n","    svr_pred = svr.predict(X.values.reshape(-1, 1))\n","    svr_mape = mean_absolute_percentage_error(y, svr_pred)\n","    svr_rmse = mean_squared_error(y, svr_pred, squared=False)\n","\n","    # Random Forest Regression\n","    rf = RandomForestRegressor()\n","    rf.fit(X.values.reshape(-1, 1), y.values.reshape(-1, 1))\n","    rf_pred = rf.predict(X.values.reshape(-1, 1))\n","    rf_mape = mean_absolute_percentage_error(y, rf_pred)\n","    rf_rmse = mean_squared_error(y, rf_pred, squared=False)\n","\n","    # Gradient Boosting Regression\n","    gbr = GradientBoostingRegressor()\n","    gbr.fit(X.values.reshape(-1, 1), y.values.reshape(-1, 1))\n","    gbr_pred = gbr.predict(X.values.reshape(-1, 1))\n","    gbr_mape = mean_absolute_percentage_error(y, gbr_pred)\n","    gbr_rmse = mean_squared_error(y, gbr_pred, squared=False)\n","\n","    # LightGBM Regression\n","    lgb_reg = lgb.LGBMRegressor()\n","    lgb_reg.fit(X.values.reshape(-1, 1), y.values.reshape(-1, 1))\n","    lgb_pred = lgb_reg.predict(X.values.reshape(-1, 1))\n","    lgb_mape = mean_absolute_percentage_error(y, lgb_pred)\n","    lgb_rmse = mean_squared_error(y, lgb_pred, squared=False)\n","\n","    # XGBoost Regression\n","    xgb_reg = xgb.XGBRegressor()\n","    xgb_reg.fit(X.values.reshape(-1, 1), y.values.reshape(-1, 1))\n","    xgb_pred = xgb_reg.predict(X.values.reshape(-1, 1))\n","    xgb_mape = mean_absolute_percentage_error(y, xgb_pred)\n","    xgb_rmse = mean_squared_error(y, xgb_pred, squared=False)\n","\n","    # Add the ML results to the DataFrame\n","    result = pd.DataFrame([\n","        {\n","            'fund name': column,\n","            'model': 'Support Vector Regression',\n","            'mape': svr_mape,\n","            'rmse': svr_rmse\n","        },\n","        {\n","            'fund name': column,\n","            'model': 'Random Forest Regression',\n","            'mape': rf_mape,\n","            'rmse': rf_rmse\n","        },\n","        {\n","            'fund name': column,\n","            'model': 'Gradient Boosting Regression',\n","            'mape': gbr_mape,\n","            'rmse': gbr_rmse\n","        },\n","        {\n","            'fund name': column,\n","            'model': 'LightGBM Regression',\n","            'mape': lgb_mape,\n","            'rmse': lgb_rmse\n","        },\n","        {\n","            'fund name': column,\n","            'model': 'XGBoost Regression',\n","            'mape': xgb_mape,\n","            'rmse': xgb_rmse\n","        }\n","    ])\n","\n","    ml_results_l = pd.concat([ml_results_l, result])\n","\n","# Export the ML results DataFrame to a CSV file\n","ml_results_l.to_csv('ml_results_longer.csv', index=False)\n","files.download('ml_results_longer.csv')\n"],"metadata":{"id":"G9PFn_qUtCkg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Autoregressive Model**"],"metadata":{"id":"vtddcut5gbNS"}},{"cell_type":"code","source":["from statsmodels.tsa.ar_model import AutoReg\n","from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n","import pandas as pd\n","from google.colab import files\n","\n","columns_to_analyze = [\n","    'VCDAX_V_N','VCSAX_V_N', 'VENAX_V_N','VFAIX_V_N', 'VGSLX_V_N','VGHCX_V_N',\t'VITAX_V_N',\t'VSPVX_V_N','VTCAX_V_N','VUIAX_V_N', 'VINAX_V_N',\t'VUIAX_P_N',\t'VGHCX_P_N', 'VFAIX_P_N', 'VSPVX_P_N', 'VITAX_P_N', 'VGSLX_P_N','VINAX_P_N',\t'VTCAX_P_N','VCSAX_P_N','VENAX_P_N','VCDAX_P_N'\n","    ]\n","\n","# Create an empty DataFrame to store AR model results\n","ar_results_l = pd.DataFrame(columns=['fund name', 'model', 'mape', 'rmse'])\n","\n","# Run autoregressive model on each column\n","for column in columns_to_analyze:\n","    X = df[column]\n","\n","    if X.isnull().any():\n","        continue\n","\n","    # AR Model\n","    ar_model = AutoReg(X, lags=1)  # Using lag=1 for simplicity, but you can try different values\n","    ar_model_fit = ar_model.fit()\n","\n","    # Predict using AR model\n","    ar_pred = ar_model_fit.predict(start=0, end=len(X)-1)\n","\n","    # Calculate MAPE and RMSE\n","    mape = mean_absolute_percentage_error(X[1:], ar_pred[1:])\n","    rmse = mean_squared_error(X[1:], ar_pred[1:], squared=False)\n","\n","    # Add AR model results to the DataFrame\n","    result = pd.DataFrame([\n","        {\n","            'fund name': column,\n","            'model': 'Autoregressive Model',\n","            'mape': mape,\n","            'rmse': rmse\n","        }\n","    ])\n","\n","    ar_results_l = pd.concat([ar_results_l, result])\n","\n","# Export the AR model results DataFrame to a CSV file\n","ar_results_l.to_csv('ar_model_results_longer.csv', index=False)\n","files.download('ar_model_results_longer.csv')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"InICYYXDEp18","executionInfo":{"status":"ok","timestamp":1689886800236,"user_tz":240,"elapsed":513,"user":{"displayName":"Aditya Jain","userId":"11504618631135712108"}},"outputId":"1060e98d-75dd-444e-bd5d-02053b63ff77"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_0ab6546f-1725-4491-bc76-c01f7938d76e\", \"ar_model_results_longer.csv\", 1542)"]},"metadata":{}}]},{"cell_type":"markdown","source":["# Yearly Analysis"],"metadata":{"id":"vHCQ06g31ZUO"}},{"cell_type":"markdown","source":["**Granger Causality**"],"metadata":{"id":"GizWh1dR2BIc"}},{"cell_type":"code","source":["from statsmodels.tsa.stattools import grangercausalitytests\n","import pandas as pd\n","from google.colab import files\n","\n","# Create an empty DataFrame to store the Granger causality test results\n","granger_results = pd.DataFrame(columns=['year', 'fund name', 'lag', 'p-value'])\n","\n","# Get unique combinations of years\n","years = df[['year']].drop_duplicates()\n","\n","# Iterate over each year\n","for index, combo in years.iterrows():\n","    year = combo['year']\n","\n","    # Filter data for the current year\n","    filtered_data = df[(df['year'] == year)]\n","\n","    # Define the list of columns to calculate correlations and perform Granger causality test with\n","    columns_to_analyze = [\n","        'VCDAX_V_N','VCSAX_V_N', 'VENAX_V_N','VFAIX_V_N', 'VGSLX_V_N','VGHCX_V_N',\t'VITAX_V_N',\t'VSPVX_V_N','VTCAX_V_N','VUIAX_V_N', 'VINAX_V_N',\t'VUIAX_P_N',\t'VGHCX_P_N', 'VFAIX_P_N', 'VSPVX_P_N', 'VITAX_P_N', 'VGSLX_P_N','VINAX_P_N',\t'VTCAX_P_N','VCSAX_P_N','VENAX_P_N','VCDAX_P_N'\n","        ]\n","\n","    # Perform Granger causality test for each column\n","    for column in columns_to_analyze:\n","        try:\n","\n","            # Perform Granger causality test with different lags\n","            max_lag = 5  # Maximum lag to test\n","            granger_test_results = grangercausalitytests(filtered_data[[column, 'compound']], maxlag=max_lag, verbose=False)\n","\n","            # Iterate over the lag values and add significant Granger causality results to the DataFrame\n","            for lag in range(1, max_lag + 1):\n","                p_value = granger_test_results[lag][0]['ssr_ftest'][1]\n","                if p_value > 0:\n","                    result = {\n","                        'year': year,\n","                        'fund name': column,\n","                        'lag': lag,\n","                        'p-value': p_value\n","                    }\n","                    granger_results = pd.concat([granger_results, pd.DataFrame(result, index=[0])])\n","\n","        except ValueError:\n","            # Handle the ValueError when x and y have length less than 2\n","            pass\n","\n","# Export the Granger causality test results DataFrame to a CSV file\n","granger_results.to_csv('granger_results_yearly.csv', index=False)\n","files.download('granger_results_yearly.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"03WQoIgY1dfI","executionInfo":{"status":"ok","timestamp":1689915176137,"user_tz":240,"elapsed":4434,"user":{"displayName":"Aditya Jain","userId":"11504618631135712108"}},"outputId":"3ddc9b01-56ed-4e22-8aa6-7b50b719e1c3"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_39ee600b-2062-4b49-8178-ad98d3efccd6\", \"granger_results_yearly.csv\", 42151)"]},"metadata":{}}]},{"cell_type":"markdown","source":["**Pearson R Correlation**"],"metadata":{"id":"Kv81wo6K4s4W"}},{"cell_type":"code","source":["from scipy.stats import pearsonr\n","import pandas as pd\n","\n","# Create an empty DataFrame to store the statistically significant results\n","pearson_results = pd.DataFrame(columns=['year', 'fund name', 'p-value', 'correlation coefficient'])\n","\n","# Get unique combinations of year\n","years = df[['year']].drop_duplicates()\n","\n","# Iterate over each year\n","for index, combo in years.iterrows():\n","    year = combo['year']\n","\n","    # Filter data for the current year\n","    filtered_data = df[(df['year'] == year)]\n","\n","    # Perform Pearson correlation analysis for each column\n","    for column in columns_to_analyze:\n","        try:\n","            # Extract the relevant series and the target variable\n","            X = filtered_data['compound']\n","            y = filtered_data[column]\n","\n","            # Calculate Pearson correlation coefficient and p-value\n","            correlation, p_value = pearsonr(X, y)\n","\n","            # Check if the correlation is statistically significant\n","            if p_value < 0.05:\n","                # Add the significant result to the DataFrame\n","                result = {\n","                    'year': year,\n","                    'fund name': column,\n","                    'p-value': p_value,\n","                    'correlation coefficient': correlation\n","                }\n","                pearson_results = pd.concat([pearson_results, pd.DataFrame(result, index=[0])])\n","        except ValueError:\n","            # Handle the ValueError when x and y have length less than 2\n","            pass\n","\n","# Export the significant results DataFrame to a CSV file\n","pearson_results.to_csv('pearson_results_yearly.csv', index=False)\n","files.download('pearson_results_yearly.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"3Ny0GVUz4sTn","executionInfo":{"status":"ok","timestamp":1689887133491,"user_tz":240,"elapsed":560,"user":{"displayName":"Aditya Jain","userId":"11504618631135712108"}},"outputId":"10384d60-0c0a-4498-d77f-e19a74fe86ac"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_823eb164-bee2-461e-8377-3a94221797c4\", \"pearson_results_yearly.csv\", 1608)"]},"metadata":{}}]},{"cell_type":"markdown","source":["**Machine Learning: SVR, LGB, XGB, GBR, Random Forest**"],"metadata":{"id":"Kq7gHE_15XIf"}},{"cell_type":"code","source":["from sklearn.svm import SVR\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","import lightgbm as lgb\n","import xgboost as xgb\n","from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n","import pandas as pd\n","from google.colab import files\n","\n","# Create an empty DataFrame to store the ML results\n","ml_results = pd.DataFrame(columns=['year', 'fund name', 'model', 'mape', 'rmse'])\n","\n","# Get unique combinations of year\n","years = df[['year']].drop_duplicates()\n","\n","# Iterate over each year\n","for index, combo in years.iterrows():\n","    year = combo['year']\n","\n","    # Filter data for the current year\n","    filtered_data = df[(df['year'] == year)]\n","\n","    # Run machine learning models on each column\n","    for column in columns_to_analyze:\n","        X = filtered_data['compound']\n","        y = filtered_data[column]\n","\n","        if X.empty or y.empty:\n","            continue\n","\n","        # Support Vector Regression\n","        svr = SVR()\n","        svr.fit(X.values.reshape(-1, 1), y.values.reshape(-1, 1))\n","        svr_pred = svr.predict(X.values.reshape(-1, 1))\n","        svr_mape = mean_absolute_percentage_error(y, svr_pred)\n","        svr_rmse = mean_squared_error(y, svr_pred, squared=False)\n","\n","        # Random Forest Regression\n","        rf = RandomForestRegressor()\n","        rf.fit(X.values.reshape(-1, 1), y.values.reshape(-1, 1))\n","        rf_pred = rf.predict(X.values.reshape(-1, 1))\n","        rf_mape = mean_absolute_percentage_error(y, rf_pred)\n","        rf_rmse = mean_squared_error(y, rf_pred, squared=False)\n","\n","        # Gradient Boosting Regression\n","        gbr = GradientBoostingRegressor()\n","        gbr.fit(X.values.reshape(-1, 1), y.values.reshape(-1, 1))\n","        gbr_pred = gbr.predict(X.values.reshape(-1, 1))\n","        gbr_mape = mean_absolute_percentage_error(y, gbr_pred)\n","        gbr_rmse = mean_squared_error(y, gbr_pred, squared=False)\n","\n","        # LightGBM Regression\n","        lgb_reg = lgb.LGBMRegressor()\n","        lgb_reg.fit(X.values.reshape(-1, 1), y.values.reshape(-1, 1))\n","        lgb_pred = lgb_reg.predict(X.values.reshape(-1, 1))\n","        lgb_mape = mean_absolute_percentage_error(y, lgb_pred)\n","        lgb_rmse = mean_squared_error(y, lgb_pred, squared=False)\n","\n","        # XGBoost Regression\n","        xgb_reg = xgb.XGBRegressor()\n","        xgb_reg.fit(X.values.reshape(-1, 1), y.values.reshape(-1, 1))\n","        xgb_pred = xgb_reg.predict(X.values.reshape(-1, 1))\n","        xgb_mape = mean_absolute_percentage_error(y, xgb_pred)\n","        xgb_rmse = mean_squared_error(y, xgb_pred, squared=False)\n","\n","        # Add the ML results to the DataFrame\n","        result = pd.DataFrame([\n","            {\n","                'year': year,\n","                'fund name': column,\n","                'model': 'Support Vector Regression',\n","                'mape': svr_mape,\n","                'rmse': svr_rmse\n","            },\n","            {\n","                'year': year,\n","                'fund name': column,\n","                'model': 'Random Forest Regression',\n","                'mape': rf_mape,\n","                'rmse': rf_rmse\n","            },\n","            {\n","                'year': year,\n","                'fund name': column,\n","                'model': 'Gradient Boosting Regression',\n","                'mape': gbr_mape,\n","                'rmse': gbr_rmse\n","            },\n","            {\n","                'year': year,\n","                'fund name': column,\n","                'model': 'LightGBM Regression',\n","                'mape': lgb_mape,\n","                'rmse': lgb_rmse\n","            },\n","            {\n","                'year': year,\n","                'fund name': column,\n","                'model': 'XGBoost Regression',\n","                'mape': xgb_mape,\n","                'rmse': xgb_rmse\n","            }\n","        ])\n","\n","        ml_results = pd.concat([ml_results, result])\n","\n","# Export the ML results DataFrame to a CSV file\n","ml_results.to_csv('ml_results_yearly.csv', index=False)\n","files.download('ml_results_yearly.csv')"],"metadata":{"id":"HkkAaDlG5WlB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Autoregressive Model**"],"metadata":{"id":"hEnD2oIegjNj"}},{"cell_type":"code","source":["from statsmodels.tsa.ar_model import AutoReg\n","from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n","import pandas as pd\n","from google.colab import files\n","\n","# Create an empty DataFrame to store the AR model results\n","ar_results = pd.DataFrame(columns=['year', 'fund name', 'model', 'mape', 'rmse'])\n","\n","# Get unique combinations of year\n","years = df[['year']].drop_duplicates()\n","\n","# Iterate over each year\n","for index, combo in years.iterrows():\n","    year = combo['year']\n","\n","    # Filter data for the current year\n","    filtered_data = df[df['year'] == year]\n","\n","    # Run autoregressive model on each column for the current year\n","    for column in columns_to_analyze:\n","        X = filtered_data[column].copy()  # Create a copy of the column data\n","\n","        if X.isnull().any():\n","            # If there are missing values, skip this column\n","            continue\n","\n","        # AR Model\n","        ar_model = AutoReg(X, lags=1)  # Using lag=1 for simplicity, but you can try different values\n","        ar_model_fit = ar_model.fit()\n","\n","        # Predict using AR model\n","        ar_pred = ar_model_fit.predict(start=0, end=len(X))\n","\n","        # Calculate MAPE and RMSE\n","        mape = mean_absolute_percentage_error(X, ar_pred[1:])\n","        rmse = mean_squared_error(X, ar_pred[1:], squared=False)\n","\n","        # Add AR model results to the DataFrame\n","        result = pd.DataFrame([\n","            {\n","                'year': year,\n","                'fund name': column,\n","                'model': 'Autoregressive Model',\n","                'mape': mape,\n","                'rmse': rmse\n","            }\n","        ])\n","\n","        ar_results = pd.concat([ar_results, result])\n","\n","# Export the AR model results DataFrame to a CSV file\n","ar_results.to_csv('ar_model_results_yearly.csv', index=False)\n","files.download('ar_model_results_yearly.csv')\n"],"metadata":{"id":"Q0BAd3T6ZlEh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Seasonal Analysis"],"metadata":{"id":"RLDe_Cfj6WLz"}},{"cell_type":"markdown","source":["**Granger Causality**"],"metadata":{"id":"dN7rIURdYOI6"}},{"cell_type":"code","source":["from statsmodels.tsa.stattools import grangercausalitytests\n","import pandas as pd\n","from google.colab import files\n","\n","# Create an empty DataFrame to store the Granger causality test results\n","granger_results = pd.DataFrame(columns=['year', 'season', 'fund name', 'lag', 'p-value'])\n","\n","# Get unique combinations of year and season\n","year_season_combos = df[['year', 'season']].drop_duplicates()\n","\n","# Iterate over each year-season combo\n","for index, combo in year_season_combos.iterrows():\n","    year = combo['year']\n","    season = combo['season']\n","\n","    # Filter data for the current year-season combo\n","    filtered_data = df[(df['year'] == year) & (df['season'] == season)]\n","\n","    # Define the list of columns to calculate correlations and perform Granger causality test with\n","    columns_to_analyze = [\n","        'VCDAX_V_N','VCSAX_V_N', 'VENAX_V_N','VFAIX_V_N', 'VGSLX_V_N','VGHCX_V_N',\t'VITAX_V_N',\t'VSPVX_V_N','VTCAX_V_N','VUIAX_V_N', 'VINAX_V_N',\t'VUIAX_P_N',\t'VGHCX_P_N', 'VFAIX_P_N', 'VSPVX_P_N', 'VITAX_P_N', 'VGSLX_P_N','VINAX_P_N',\t'VTCAX_P_N','VCSAX_P_N','VENAX_P_N','VCDAX_P_N'\n","        ]\n","\n","    # Perform Granger causality test for each column\n","    for column in columns_to_analyze:\n","        try:\n","\n","            # Perform Granger causality test with different lags\n","            max_lag = 5  # Maximum lag to test\n","            granger_test_results = grangercausalitytests(filtered_data[[column, 'compound']], maxlag=max_lag, verbose=False)\n","\n","            # Iterate over the lag values and add significant Granger causality results to the DataFrame\n","            for lag in range(1, max_lag + 1):\n","                p_value = granger_test_results[lag][0]['ssr_ftest'][1]\n","                if p_value > 0:\n","                    result = {\n","                        'year': year,\n","                        'season': season,\n","                        'fund name': column,\n","                        'lag': lag,\n","                        'p-value': p_value\n","                    }\n","                    granger_results = pd.concat([granger_results, pd.DataFrame(result, index=[0])])\n","\n","        except ValueError:\n","            # Handle the ValueError when x and y have length less than 2\n","            pass\n","\n","# Export the Granger causality test results DataFrame to a CSV file\n","granger_results.to_csv('granger_results_shorter.csv', index=False)\n","files.download('granger_results_shorter.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"Z_4fd9IxEJvX","executionInfo":{"status":"ok","timestamp":1689915171711,"user_tz":240,"elapsed":38239,"user":{"displayName":"Aditya Jain","userId":"11504618631135712108"}},"outputId":"375b0a1c-408f-442e-f74c-a776c8677f0b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_16a3cb54-0319-48f6-b9dd-cb1ae5593c15\", \"granger_results_shorter.csv\", 187229)"]},"metadata":{}}]},{"cell_type":"markdown","source":["**Pearson R Correlation**"],"metadata":{"id":"_Ui_WUsuaLJD"}},{"cell_type":"code","source":["from scipy.stats import pearsonr\n","import pandas as pd\n","\n","# Create an empty DataFrame to store the statistically significant results\n","pearson_results = pd.DataFrame(columns=['year', 'season', 'fund name', 'p-value', 'correlation coefficient'])\n","\n","# Get unique combinations of year and season\n","year_season_combos = df[['year', 'season']].drop_duplicates()\n","\n","# Iterate over each year-season combo\n","for index, combo in year_season_combos.iterrows():\n","    year = combo['year']\n","    season = combo['season']\n","\n","    # Filter data for the current year-season combo\n","    filtered_data = df[(df['year'] == year) & (df['season'] == season)]\n","\n","    # Perform Pearson correlation analysis for each column\n","    for column in columns_to_analyze:\n","        try:\n","            # Extract the relevant series and the target variable\n","            X = filtered_data['compound']\n","            y = filtered_data[column]\n","\n","            # Calculate Pearson correlation coefficient and p-value\n","            correlation, p_value = pearsonr(X, y)\n","\n","            # Check if the correlation is statistically significant\n","            if p_value < 0.05:\n","                # Add the significant result to the DataFrame\n","                result = {\n","                    'year': year,\n","                    'season': season,\n","                    'fund name': column,\n","                    'p-value': p_value,\n","                    'correlation coefficient': correlation\n","                }\n","                pearson_results = pd.concat([pearson_results, pd.DataFrame(result, index=[0])])\n","        except ValueError:\n","            # Handle the ValueError when x and y have length less than 2\n","            pass\n","\n","# Export the significant results DataFrame to a CSV file\n","pearson_results.to_csv('pearson_results_shorter.csv', index=False)\n","files.download('pearson_results_shorter.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"jAxaRuNTaSUa","executionInfo":{"status":"ok","timestamp":1689696462442,"user_tz":240,"elapsed":1494,"user":{"displayName":"Aditya Jain","userId":"11504618631135712108"}},"outputId":"bba9be5f-d503-489a-be0e-321d8f92f5dd"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_ce9fb4ea-396e-4e36-8734-dea93d89fea5\", \"pearson_results_shorter.csv\", 5163)"]},"metadata":{}}]},{"cell_type":"markdown","source":["**Machine Learning: SVR, LGB, XGB, GBR, Random Forest**"],"metadata":{"id":"0s1w-IRCgHEW"}},{"cell_type":"code","source":["from sklearn.svm import SVR\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","import lightgbm as lgb\n","import xgboost as xgb\n","from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n","import pandas as pd\n","from google.colab import files\n","\n","# Create an empty DataFrame to store the ML results\n","ml_results = pd.DataFrame(columns=['year', 'season', 'fund name', 'model', 'mape', 'rmse'])\n","\n","# Get unique combinations of year and season\n","year_season_combos = df[['year', 'season']].drop_duplicates()\n","\n","# Iterate over each year-season combo\n","for index, combo in year_season_combos.iterrows():\n","    year = combo['year']\n","    season = combo['season']\n","\n","    # Filter data for the current year-season combo\n","    filtered_data = df[(df['year'] == year) & (df['season'] == season)]\n","\n","    # Run machine learning models on each column\n","    for column in columns_to_analyze:\n","        X = filtered_data['compound']\n","        y = filtered_data[column]\n","\n","        if X.empty or y.empty:\n","            continue\n","\n","        # Support Vector Regression\n","        svr = SVR()\n","        svr.fit(X.values.reshape(-1, 1), y.values.reshape(-1, 1))\n","        svr_pred = svr.predict(X.values.reshape(-1, 1))\n","        svr_mape = mean_absolute_percentage_error(y, svr_pred)\n","        svr_rmse = mean_squared_error(y, svr_pred, squared=False)\n","\n","        # Random Forest Regression\n","        rf = RandomForestRegressor()\n","        rf.fit(X.values.reshape(-1, 1), y.values.reshape(-1, 1))\n","        rf_pred = rf.predict(X.values.reshape(-1, 1))\n","        rf_mape = mean_absolute_percentage_error(y, rf_pred)\n","        rf_rmse = mean_squared_error(y, rf_pred, squared=False)\n","\n","        # Gradient Boosting Regression\n","        gbr = GradientBoostingRegressor()\n","        gbr.fit(X.values.reshape(-1, 1), y.values.reshape(-1, 1))\n","        gbr_pred = gbr.predict(X.values.reshape(-1, 1))\n","        gbr_mape = mean_absolute_percentage_error(y, gbr_pred)\n","        gbr_rmse = mean_squared_error(y, gbr_pred, squared=False)\n","\n","        # LightGBM Regression\n","        lgb_reg = lgb.LGBMRegressor()\n","        lgb_reg.fit(X.values.reshape(-1, 1), y.values.reshape(-1, 1))\n","        lgb_pred = lgb_reg.predict(X.values.reshape(-1, 1))\n","        lgb_mape = mean_absolute_percentage_error(y, lgb_pred)\n","        lgb_rmse = mean_squared_error(y, lgb_pred, squared=False)\n","\n","        # XGBoost Regression\n","        xgb_reg = xgb.XGBRegressor()\n","        xgb_reg.fit(X.values.reshape(-1, 1), y.values.reshape(-1, 1))\n","        xgb_pred = xgb_reg.predict(X.values.reshape(-1, 1))\n","        xgb_mape = mean_absolute_percentage_error(y, xgb_pred)\n","        xgb_rmse = mean_squared_error(y, xgb_pred, squared=False)\n","\n","        # Add the ML results to the DataFrame\n","        result = pd.DataFrame([\n","            {\n","                'year': year,\n","                'season': season,\n","                'fund name': column,\n","                'model': 'Support Vector Regression',\n","                'mape': svr_mape,\n","                'rmse': svr_rmse\n","            },\n","            {\n","                'year': year,\n","                'season': season,\n","                'fund name': column,\n","                'model': 'Random Forest Regression',\n","                'mape': rf_mape,\n","                'rmse': rf_rmse\n","            },\n","            {\n","                'year': year,\n","                'season': season,\n","                'fund name': column,\n","                'model': 'Gradient Boosting Regression',\n","                'mape': gbr_mape,\n","                'rmse': gbr_rmse\n","            },\n","            {\n","                'year': year,\n","                'season': season,\n","                'fund name': column,\n","                'model': 'LightGBM Regression',\n","                'mape': lgb_mape,\n","                'rmse': lgb_rmse\n","            },\n","            {\n","                'year': year,\n","                'season': season,\n","                'fund name': column,\n","                'model': 'XGBoost Regression',\n","                'mape': xgb_mape,\n","                'rmse': xgb_rmse\n","            }\n","        ])\n","\n","        ml_results = pd.concat([ml_results, result])\n","\n","# Export the ML results DataFrame to a CSV file\n","ml_results.to_csv('ml_results_shorter.csv', index=False)\n","files.download('ml_results_shorter.csv')\n"],"metadata":{"id":"q2iARQxBst47"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Autoregressive Model**"],"metadata":{"id":"omBIsW8nkR65"}},{"cell_type":"code","source":["from statsmodels.tsa.ar_model import AutoReg\n","from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n","import pandas as pd\n","from google.colab import files\n","\n","# Create an empty DataFrame to store AR model results for each year, season, and column\n","ar_results_year_season = pd.DataFrame(columns=['year', 'season', 'fund name', 'model', 'mape', 'rmse'])\n","\n","# Get unique combinations of year and season\n","year_season_combos = df[['year', 'season']].drop_duplicates()\n","\n","# Iterate over each year-season combo\n","for index, combo in year_season_combos.iterrows():\n","    year = combo['year']\n","    season = combo['season']\n","\n","    # Filter data for the current year-season combo\n","    filtered_data = df[(df['year'] == year) & (df['season'] == season)]\n","\n","    # Run autoregressive model on each column for the current year-season combo\n","    for column in columns_to_analyze:\n","        X = filtered_data[column].copy()\n","\n","        if X.empty:\n","            continue\n","\n","        # AR Model\n","        ar_model = AutoReg(X, lags=1)  # Using lag=1 for simplicity, but you can try different values\n","        ar_model_fit = ar_model.fit()\n","\n","        # Predict using AR model\n","        ar_pred = ar_model_fit.predict(start=0, end=len(X))\n","\n","        # Calculate MAPE and RMSE\n","        mape = mean_absolute_percentage_error(X, ar_pred[1:])\n","        rmse = mean_squared_error(X, ar_pred[1:], squared=False)\n","\n","        # Add AR model results to the DataFrame\n","        result = pd.DataFrame([\n","            {\n","                'year': year,\n","                'season': season,\n","                'fund name': column,\n","                'model': 'Autoregressive Model',\n","                'mape': mape,\n","                'rmse': rmse\n","            }\n","        ])\n","\n","        ar_results_year_season = pd.concat([ar_results_year_season, result])\n","\n","# Export the AR model results DataFrame to a CSV file\n","ar_results_year_season.to_csv('ar_model_results_shorter.csv', index=False)\n","files.download('ar_model_results_shorter.csv')\n"],"metadata":{"id":"Nu_y1-6TkQu4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Graphs"],"metadata":{"id":"lwv51qxLpy38"}}]}